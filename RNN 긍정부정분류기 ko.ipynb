{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id                                           document  label\n",
      "0        8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
      "1        8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
      "2        4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
      "3        9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
      "4       10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
      "...          ...                                                ...    ...\n",
      "199995   8963373                                     포켓 몬스터 짜가 ㅡㅡ;;      0\n",
      "199996   3302770                                              쓰.레.기      0\n",
      "199997   5458175                  완전 사이코영화. 마지막은 더욱더 이 영화의질을 떨어트린다.      0\n",
      "199998   6908648                왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ      0\n",
      "199999   8548411                                    포풍저그가나가신다영차영차영차      0\n",
      "\n",
      "[200000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "naver_data = pd.read_table('./data/ratings.txt')\n",
    "print(naver_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id                                           document  label\n",
      "0        8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
      "1        8132799  디자인을 배우는 학생으로  외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
      "2        4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음   최고       1\n",
      "3        9251303  와   연기가 진짜 개쩔구나   지루할거라고 생각했는데 몰입해서 봤다   그래 이런...      1\n",
      "4       10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화       1\n",
      "...          ...                                                ...    ...\n",
      "199995   8963373                                     포켓 몬스터 짜가 ㅡㅡ        0\n",
      "199996   3302770                                              쓰 레 기      0\n",
      "199997   5458175                  완전 사이코영화  마지막은 더욱더 이 영화의질을 떨어트린다       0\n",
      "199998   6908648                왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ      0\n",
      "199999   8548411                                    포풍저그가나가신다영차영차영차      0\n",
      "\n",
      "[200000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식을 사용하여 단어가 아닌것을 삭제\n",
    "\n",
    "naver_data['document'] = naver_data['document'].str.replace(\"[^\\w]|br\", \" \")\n",
    "print(naver_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150000,), (50000,), (150000,), (50000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "document_train, document_test, y_train, y_test = train_test_split(naver_data['document'], naver_data['label'], test_size = 0.25, shuffle=True, random_state=3)\n",
    "document_train.shape, document_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No matching overloads found for kr.lucypark.okt.OktInterface.tokenize(float,java.lang.Boolean,java.lang.Boolean), options are:\n\tpublic java.util.List kr.lucypark.okt.OktInterface.tokenize(java.lang.String,java.lang.Boolean,java.lang.Boolean)\n\n\tat JPMethod::findOverload(native\\common\\jp_method.cpp:242)\n\tat JPMethod::findOverload(native\\common\\jp_method.cpp:245)\n\tat JPMethod::invoke(native\\common\\jp_method.cpp:253)\n\tat PyJPMethod::__call__(native\\python\\pyjp_method.cpp:142)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4b18db63663e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOkt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py\u001b[0m in \u001b[0;36mmorphs\u001b[1;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;34m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mphrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBoolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                     jpype.java.lang.Boolean(stem)).toArray()\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: No matching overloads found for kr.lucypark.okt.OktInterface.tokenize(float,java.lang.Boolean,java.lang.Boolean), options are:\n\tpublic java.util.List kr.lucypark.okt.OktInterface.tokenize(java.lang.String,java.lang.Boolean,java.lang.Boolean)\n\n\tat JPMethod::findOverload(native\\common\\jp_method.cpp:242)\n\tat JPMethod::findOverload(native\\common\\jp_method.cpp:245)\n\tat JPMethod::invoke(native\\common\\jp_method.cpp:253)\n\tat PyJPMethod::__call__(native\\python\\pyjp_method.cpp:142)\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 예시용 stopwords (불용어)\n",
    "stopwords = ['을', '를', '이', '가', '으로', '에게']\n",
    "\n",
    "# 토큰화 진행\n",
    "\n",
    "X_train = []\n",
    "for stc in document_train:\n",
    "    token = []\n",
    "    words = Okt().morphs(stc)\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            token.append(word)\n",
    "    X_train.append(token)\n",
    "\n",
    "# X_test = []\n",
    "# for stc in review_test:\n",
    "#     token = []\n",
    "#     words = Okt().morphs(stc)\n",
    "#     for word in words:\n",
    "#         if word not in stopwords:\n",
    "#             token.append(word)\n",
    "#     X_test.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# fit on texts는 train기준으로 한번만 한다. \n",
    "# train에 해당되지 않는 단어는 사라진다.\n",
    "# 데이터가 사라지는게 싫으면 전체데이터를 stopwords한 다음 fit_on_text를 진행한다.\n",
    "# 안에 값을 넣어주면 해당되는 값만큼 인덱스를 부여한다.\n",
    "# 무의미하게 아무렇게 단어를 고르는게 아니라 단어 빈도수가 너무 적은 필요없는 단어를 제외하고 필요한 단어 5000개만 잡아서 넣어준다.\n",
    "# 데이터를 분석하여 유의미한 데이터 갯수를 파악한 뒤어 사용해야한다. 그냥 사용하다가는 의미있는 데이터가 날라간다.\n",
    "\n",
    "tokenizer = Tokenizer(5000) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# 동일한 단어는 동일한 인덱스\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
