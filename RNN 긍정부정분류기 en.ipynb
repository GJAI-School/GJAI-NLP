{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "imdb_data = pd.read_csv('./data/IMDB Dataset.csv')\n",
    "print(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review  sentiment\n",
      "0      One of the other reviewers has mentioned that ...          1\n",
      "1      A wonderful little production. <br /><br />The...          1\n",
      "2      I thought this was a wonderful way to spend ti...          1\n",
      "3      Basically there's a family where a little boy ...          0\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...          1\n",
      "...                                                  ...        ...\n",
      "49995  I thought this movie did a down right good job...          1\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...          0\n",
      "49997  I am a Catholic taught in parochial elementary...          0\n",
      "49998  I'm going to have to disagree with the previou...          0\n",
      "49999  No one expects the Star Trek movies to be high...          0\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# pos, neg 값을 숫자로 변경하는 작업\n",
    "\n",
    "imdb_data['sentiment'] = imdb_data['sentiment'].replace('positive', 1)\n",
    "imdb_data['sentiment'] = imdb_data['sentiment'].replace('negative', 0)\n",
    "print(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review  sentiment\n",
      "0      One of the other reviewers has mentioned that ...          1\n",
      "1      A wonderful little production            The f...          1\n",
      "2      I thought this was a wonderful way to spend ti...          1\n",
      "3      Basically there s a family where a little boy ...          0\n",
      "4      Petter Mattei s  Love in the Time of Money  is...          1\n",
      "...                                                  ...        ...\n",
      "49995  I thought this movie did a down right good job...          1\n",
      "49996  Bad plot  bad dialogue  bad acting  idiotic di...          0\n",
      "49997  I am a Catholic taught in parochial elementary...          0\n",
      "49998  I m going to have to disagree with the previou...          0\n",
      "49999  No one expects the Star Trek movies to be high...          0\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식을 사용하여 단어가 아닌것을 삭제\n",
    "\n",
    "imdb_data['review'] = imdb_data['review'].str.replace(\"[^\\w]|br\", \" \")\n",
    "print(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review  sentiment\n",
      "0      One of the other reviewers has mentioned that ...          1\n",
      "1      A wonderful little production            The f...          1\n",
      "2      I thought this was a wonderful way to spend ti...          1\n",
      "3      Basically there s a family where a little boy ...          0\n",
      "4      Petter Mattei s  Love in the Time of Money  is...          1\n",
      "...                                                  ...        ...\n",
      "49995  I thought this movie did a down right good job...          1\n",
      "49996  Bad plot  bad dialogue  bad acting  idiotic di...          0\n",
      "49997  I am a Catholic taught in parochial elementary...          0\n",
      "49998  I m going to have to disagree with the previou...          0\n",
      "49999  No one expects the Star Trek movies to be high...          0\n",
      "\n",
      "[50000 rows x 2 columns]\n",
      "                                                  review  sentiment\n",
      "0      One of the other reviewers has mentioned that ...          1\n",
      "1      A wonderful little production            The f...          1\n",
      "2      I thought this was a wonderful way to spend ti...          1\n",
      "3      Basically there s a family where a little boy ...          0\n",
      "4      Petter Mattei s  Love in the Time of Money  is...          1\n",
      "...                                                  ...        ...\n",
      "49995  I thought this movie did a down right good job...          1\n",
      "49996  Bad plot  bad dialogue  bad acting  idiotic di...          0\n",
      "49997  I am a Catholic taught in parochial elementary...          0\n",
      "49998  I m going to have to disagree with the previou...          0\n",
      "49999  No one expects the Star Trek movies to be high...          0\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 데이터에 따른 선택사항\n",
    "# 혹시 전처리 이후 공백으로만 이루어진 데이터가 있을 경우 삭제를 할 필요가 있다.\n",
    "imdb_data['review'] = imdb_data['review'].replace('', np.nan)\n",
    "imdb_data['sentiment'] = imdb_data['sentiment'].replace('', np.nan)\n",
    "print(imdb_data)\n",
    "\n",
    "imdb_data = imdb_data.dropna(how='any')\n",
    "print(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500,), (12500,), (37500,), (12500,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "review_train, review_test, y_train, y_test = train_test_split(imdb_data['review'], imdb_data['sentiment'], test_size = 0.25, shuffle=True, random_state=3)\n",
    "review_train.shape, review_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시용 stopwords (불용어)\n",
    "stopwords = ['a', 'an', 'the', 'very']\n",
    "\n",
    "# 토큰화 진행\n",
    "\n",
    "X_train = []\n",
    "for stc in review_train:\n",
    "    token = []\n",
    "    words = stc.split()\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            token.append(word)\n",
    "    X_train.append(token)\n",
    "\n",
    "X_test = []\n",
    "for stc in review_test:\n",
    "    token = []\n",
    "    words = stc.split()\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            token.append(word)\n",
    "    X_test.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'was',\n",
       " 'really',\n",
       " 'nightmare',\n",
       " 'of',\n",
       " 'film',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'it',\n",
       " 'about',\n",
       " 'nine',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'on',\n",
       " 'cable',\n",
       " 'TV',\n",
       " 'and',\n",
       " 'haven',\n",
       " 't',\n",
       " 'forgotten',\n",
       " 'it',\n",
       " 'since',\n",
       " 'Pixote',\n",
       " 'is',\n",
       " '10',\n",
       " 'year',\n",
       " 'old',\n",
       " 'boy',\n",
       " 'who',\n",
       " 'lives',\n",
       " 'in',\n",
       " 'the',\n",
       " 'streets',\n",
       " 'of',\n",
       " 'Sao',\n",
       " 'Paulo',\n",
       " 'Brazil',\n",
       " 'and',\n",
       " 'leads',\n",
       " 'criminal',\n",
       " 'life',\n",
       " 'in',\n",
       " 'the',\n",
       " 'company',\n",
       " 'of',\n",
       " 'his',\n",
       " 'teenage',\n",
       " 'friends',\n",
       " 'Lilica',\n",
       " 'Dito',\n",
       " 'and',\n",
       " 'Chico',\n",
       " 'they',\n",
       " 'steal',\n",
       " 'pimp',\n",
       " 'sell',\n",
       " 'drugs',\n",
       " 'and',\n",
       " 'murder',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'survive',\n",
       " 'each',\n",
       " 'day',\n",
       " 'In',\n",
       " 'the',\n",
       " 'first',\n",
       " 'half',\n",
       " 'of',\n",
       " 'the',\n",
       " 'film',\n",
       " 'Pixote',\n",
       " 'is',\n",
       " 'caught',\n",
       " 'by',\n",
       " 'the',\n",
       " 'police',\n",
       " 'and',\n",
       " 'sent',\n",
       " 'to',\n",
       " 'sadistic',\n",
       " 'foster',\n",
       " 'home',\n",
       " 'where',\n",
       " 'he',\n",
       " 'witnesses',\n",
       " 'every',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'from',\n",
       " 'the',\n",
       " 'older',\n",
       " 'inmates',\n",
       " 'and',\n",
       " 'guards',\n",
       " 'to',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'kids',\n",
       " 'one',\n",
       " 'night',\n",
       " 'Lilica',\n",
       " 's',\n",
       " 'boyfriend',\n",
       " 'is',\n",
       " 'killed',\n",
       " 'after',\n",
       " 'beating',\n",
       " 'so',\n",
       " 'Pixote',\n",
       " 'and',\n",
       " 'his',\n",
       " 'friends',\n",
       " 'decide',\n",
       " 'to',\n",
       " 'escape',\n",
       " 'during',\n",
       " 'riot',\n",
       " 'The',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'film',\n",
       " 'shows',\n",
       " 'Pixote',\n",
       " 's',\n",
       " 'descent',\n",
       " 'into',\n",
       " 'criminal',\n",
       " 'life',\n",
       " 'he',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'show',\n",
       " 'any',\n",
       " 'feelings',\n",
       " 'or',\n",
       " 'remorse',\n",
       " 'after',\n",
       " 'killing',\n",
       " 'someone',\n",
       " 'maybe',\n",
       " 'because',\n",
       " 'he',\n",
       " 'knows',\n",
       " 'that',\n",
       " 'good',\n",
       " 'feelings',\n",
       " 'are',\n",
       " 'of',\n",
       " 'no',\n",
       " 'use',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'in',\n",
       " 'which',\n",
       " 'he',\n",
       " 'lives',\n",
       " 'But',\n",
       " 'there',\n",
       " 'is',\n",
       " 'however',\n",
       " 'gentle',\n",
       " 'scene',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'film',\n",
       " 'Pixote',\n",
       " 'and',\n",
       " 'his',\n",
       " 'friends',\n",
       " 'are',\n",
       " 'at',\n",
       " 'the',\n",
       " 'beach',\n",
       " 'missing',\n",
       " 'and',\n",
       " 'wishing',\n",
       " 'one',\n",
       " 'of',\n",
       " 'his',\n",
       " 'friends',\n",
       " 'from',\n",
       " 'the',\n",
       " 'reformatory',\n",
       " 'was',\n",
       " 'there',\n",
       " 'I',\n",
       " 'thought',\n",
       " 'it',\n",
       " 'was',\n",
       " 'poetic',\n",
       " 'and',\n",
       " 'melancholy',\n",
       " 'scene',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'all',\n",
       " 'these',\n",
       " 'horrible',\n",
       " 'events',\n",
       " 'the',\n",
       " 'boys',\n",
       " 'are',\n",
       " 'obviously',\n",
       " 'longing',\n",
       " 'not',\n",
       " 'only',\n",
       " 'for',\n",
       " 'their',\n",
       " 'friend',\n",
       " 'but',\n",
       " 'for',\n",
       " 'better',\n",
       " 'life',\n",
       " 'Director',\n",
       " 'Hector',\n",
       " 'Babenco',\n",
       " 's',\n",
       " 'Pixote',\n",
       " 'is',\n",
       " 'ave',\n",
       " 'and',\n",
       " 'depressing',\n",
       " 'film',\n",
       " 'that',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'shy',\n",
       " 'away',\n",
       " 'from',\n",
       " 'showing',\n",
       " 'the',\n",
       " 'harshest',\n",
       " 'reality',\n",
       " 'many',\n",
       " 'people',\n",
       " 'including',\n",
       " 'myself',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'ignore',\n",
       " 'or',\n",
       " 'misunderstand',\n",
       " 'This',\n",
       " 'film',\n",
       " 'will',\n",
       " 'probably',\n",
       " 'open',\n",
       " 'your',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'make',\n",
       " 'you',\n",
       " 'better',\n",
       " 'and',\n",
       " 'compassionate',\n",
       " 'person']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# fit on texts는 train기준으로 한번만 한다. \n",
    "# train에 해당되지 않는 단어는 사라진다.\n",
    "# 데이터가 사라지는게 싫으면 전체데이터를 stopwords한 다음 fit_on_text를 진행한다.\n",
    "# 안에 값을 넣어주면 해당되는 값만큼 인덱스를 부여한다.\n",
    "# 무의미하게 아무렇게 단어를 고르는게 아니라 단어 빈도수가 너무 적은 필요없는 단어를 제외하고 필요한 단어 5000개만 잡아서 넣어준다.\n",
    "# 데이터를 분석하여 유의미한 데이터 갯수를 파악한 뒤어 사용해야한다. 그냥 사용하다가는 의미있는 데이터가 날라간다.\n",
    "\n",
    "tokenizer = Tokenizer(5000) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# 동일한 단어는 동일한 인덱스\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 1589, 9, 14, 98, 2377, 1323, 582, 636, 137, 4, 1, 924, 3, 691, 396, 21, 10, 1389, 3, 1, 115, 115, 69, 49, 8, 646, 183, 1, 262, 2, 6, 1191, 1, 327, 5660, 3, 587, 69, 8, 65, 4, 791, 6, 39, 4, 62, 1, 6100, 3, 401, 4550, 30, 40, 480, 4, 2336, 40, 70, 480, 81, 14069, 5182, 8, 288, 20, 1000, 29541, 38, 45366, 16, 517, 2, 12573, 226, 17, 8, 12, 1000, 30, 218, 14070, 317, 3, 45, 473, 4, 28, 1, 281, 2609, 3, 9, 14, 69, 1059, 37, 288, 20, 233, 36, 10, 1, 327, 5660, 3, 325, 174, 540, 203, 35, 1, 456, 2, 91, 180, 72, 4441, 35, 37, 163, 61, 540, 7, 9, 634, 6, 12, 180, 2810, 3439, 2, 15454, 22, 73, 146, 17, 37, 5, 3787, 7, 9, 27, 19, 222, 521, 86, 4, 26, 3927, 21, 1, 806, 3, 9, 634, 17, 23, 150, 20, 59, 58, 3, 75, 334, 4, 26, 761, 38, 291, 16, 9, 3885, 37, 12, 54, 144, 291, 4, 26, 3787, 37, 143, 3, 75, 39, 2967, 4023, 46, 3, 1, 340, 586, 133, 6, 150, 20, 61, 1251, 1, 462, 3, 132, 14071, 344, 6, 277, 172, 382, 3, 46, 937, 133, 59, 439, 221, 17, 1, 602, 210, 20, 13, 78, 13, 6, 891, 16, 1971, 1511, 148, 40, 480, 15455, 15, 629, 32094, 2, 35173, 16, 1, 84, 55, 30, 1023, 1118, 300, 3, 315, 40, 2028, 4, 1, 4156, 31, 115, 40, 11893, 4, 1, 364, 300, 3, 131, 2554, 299, 21, 38, 39, 159, 6981, 40, 3347, 56271, 6333, 1667, 9, 27, 936, 20, 1, 2273, 826, 12, 6615, 4, 879, 15, 163, 366, 3303, 38, 221, 61, 540, 54, 1183, 4, 172, 3, 1, 50, 221, 554, 54, 144, 220, 38, 458, 194, 4, 8389, 4156, 1990, 290, 916, 2, 91, 2, 91, 677, 678, 1241, 1558, 122, 3, 653, 1, 11455, 4156, 1667, 9, 5, 180, 346, 1, 125, 9253, 24, 30, 1, 456, 2, 1353, 435, 340, 495, 2, 1611, 42, 19, 26, 105, 925, 4156, 2642, 4156, 38, 56, 8389, 4156, 5090, 71, 9, 4209, 1452, 2, 4420, 19, 42, 19, 176, 4156, 83, 20, 92, 1, 168, 1349, 8, 117, 2, 62, 9, 27, 139, 74, 46, 45366, 29541, 38, 56272, 5090, 300, 1, 60, 146, 8, 186, 41, 3, 9, 634, 12, 85, 358, 77, 48, 28, 22, 16, 742, 532, 12102, 38, 402, 1728, 3017, 17, 64, 8770, 16, 7588, 132, 1384, 1, 56273, 304, 358, 16, 4156, 87, 3, 90, 24, 27658, 2, 235, 26, 172, 1259, 7, 64, 398, 2, 4156, 5, 531, 3, 7665, 16, 90, 71, 1, 9784, 304, 56, 50, 3084, 6101, 677, 188, 1, 219, 42, 3611, 2227, 1450, 1729, 2829, 1729, 8770, 4, 5412, 15, 1, 191, 10, 64, 3578, 24, 677, 834, 529, 370, 4783, 2704, 1435, 64, 1493, 7, 64, 1728, 3017, 82, 91, 4551, 4, 7447, 6786, 2, 4, 3098, 11685, 402, 1426, 59, 5, 1949, 152, 234, 10, 2399, 545, 273, 7, 3871, 501, 114, 36, 77, 420, 831, 2020, 35, 27, 157, 77, 24, 358, 1592, 45, 8, 19787, 35, 9, 634, 42, 19, 176, 4, 62, 1, 8770, 2, 2310, 4, 278, 45367, 529, 7, 4156, 2, 677, 45, 1, 141, 3, 9, 262, 4610, 74, 56274, 1, 111, 3, 1887, 17820, 91, 50, 495, 2, 221, 71, 1, 8389, 2, 170, 50, 424, 422, 2, 91, 159, 47]\n",
      "47918    1\n",
      "4998     0\n",
      "27452    1\n",
      "33794    0\n",
      "7252     1\n",
      "        ..\n",
      "25544    1\n",
      "48056    1\n",
      "11513    1\n",
      "1688     0\n",
      "5994     0\n",
      "Name: sentiment, Length: 37500, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 길이를 맞춰준다\n",
    "# 임의로 맞추는게 아니라 데이터셋을 보면서 최대 문장 길이가 얼마인지 확인하고 거기에 맞춰서 넣어줘야한다\n",
    "# 평균으로 맞추거나 최대길이로 맞춘다 보통\n",
    "max_len = 500\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 120)) \n",
    "model.add(LSTM(120)) \n",
    "model.add(Dense(1, activation='sigmoid')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5) # loss가 증가할때 스탑하겠다. 5번 기다리겠음\n",
    "model_check = ModelCheckpoint('the_best.h5',monitor='val_acc', mode='max', verbose=1, save_best_only=True) # acc가 가장 높을 때 확인 하겠다. 저장은 베스트일 때만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64, callbacks=[early_stop, model_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
